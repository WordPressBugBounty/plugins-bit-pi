import{aa as m,ab as r}from"./_applist-chatty-months-shout-814.js";import{_ as t}from"./main-chatty-months-shout.js";import{h as d,c as l}from"./commonut-chatty-months-shout-164.js";import{f as i}from"./machineh-chatty-months-shout-534.js";import{h as a,s}from"./machine.-chatty-months-shout-207.js";import"./lodash-chatty-months-shout-731.js";globalThis.jotaiAtomCache=globalThis.jotaiAtomCache||{cache:new Map,get(o,e){return this.cache.has(o)?this.cache.get(o):(this.cache.set(o,e),e)}};const C=m(({helpers:o})=>({states:{components:[l(o),{componentName:o.componentName.select,helperText:t("Choose the Groq model to use here. E.g. llama-3.3-70b-versatile."),id:"model-id",label:t("Model"),onChange:"HANDLE_MODEL_CHANGE",onRefetch:"REFETCH_MODELS",optionFilterProp:"label",options:[],path:"model",render:"IF_CONNECTION_SELECTED",required:!0,showSearch:!0,value:void 0},{componentName:o.componentName.mixInput,helperText:t("The maximum number of tokens that can be generated in the chat completion. If empty, the limit of the model will be used."),id:"max-tokens",label:t("Max Tokens Returned"),onChange:"SET_MAX_TOKENS",path:"max_tokens",render:"IF_MODEL_SELECTED",value:[]},{componentName:o.componentName.switch,fieldType:"config",id:"show-advance-feature",label:t("Show Advanced Settings"),onChange:"SET_FEATURE",path:"advance-feature",render:"IF_MODEL_SELECTED",size:"small",value:!1},{componentName:o.componentName.mixInput,helperText:t("Controls randomness in the response. Higher values (e.g., 0.8) make output more random and creative. Lower values (e.g., 0.2) make it more focused and deterministic. Range: 0-2, Default: 1."),id:"temperature",label:t("Temperature"),onChange:"SET_TEMPERATURE",path:"temperature",render:"IF_ADVANCE_FEATURE_SELECTED",value:[]},{componentName:o.componentName.mixInput,helperText:t("An alternative to temperature for controlling randomness. Top-p sampling considers only the top tokens whose cumulative probability mass is p. For example, 0.1 means only tokens in the top 10% probability are considered. Range: 0-1, Default: 1."),id:"top-p",label:t("Top P"),onChange:"SET_TOP_P",path:"top_p",render:"IF_ADVANCE_FEATURE_SELECTED",value:[]},{componentName:o.componentName.mixInput,helperText:t("Positive values penalize tokens based on their frequency in the text so far, decreasing the likelihood of repeated phrases. Range: -2.0 to 2.0, Default: 0."),id:"frequency-penalty",label:t("Frequency Penalty"),onChange:"SET_FREQUENCY_PENALTY",path:"frequency_penalty",render:"IF_ADVANCE_FEATURE_SELECTED",value:[]},{componentName:o.componentName.mixInput,helperText:t("If specified, the system will attempt to sample deterministically such that repeated requests with the same seed and parameters return the same result. Useful for reproducibility."),id:"seed",label:t("Seed"),onChange:"SET_SEED",path:"seed",render:"IF_ADVANCE_FEATURE_SELECTED",value:[]}]},actions:{CONNECTION_ADD_CHANGE:d,HANDLE_MODEL_CHANGE:a("modelId"),HANDLE_RESPONSE_FORMAT:a("responseFormat"),IF_CONNECTION_SELECTED:({$:e})=>{var n;return r((n=e.getComponent("connection-id"))==null?void 0:n.value)},IF_MODEL_SELECTED:({$:e})=>{var n;return r((n=e.getComponent("model-id"))==null?void 0:n.value)},REFETCH_MODELS:({$:e})=>i(e),SET_FEATURE:a("showAdvanceFeature"),SET_FREQUENCY_PENALTY:a("frequencyPenalty"),SET_MAX_TOKENS:a("maxTokens"),SET_SEED:a("seed"),SET_TEMPERATURE:a("temperature"),SET_TOP_P:a("topP"),IF_ADVANCE_FEATURE_SELECTED:({$:e})=>{var n,E;return r((n=e.getComponent("connection-id"))==null?void 0:n.value)&&((E=e.getComponent("show-advance-feature"))==null?void 0:E.value)===!0},SET_CONNECTION:({$:e,e:n})=>{a("connectionId")({$:e,e:n}),i(e)},ON_MACHINE_LOAD:({$:e})=>{s(e,[{db:"connectionId",id:"connection-id"},{db:"modelId",id:"model-id"},{db:"maxTokens",id:"max-tokens"},{db:"contextLength",id:"context-length"},{db:"temperature",id:"temperature"},{db:"topP",id:"top-p"},{db:"frequencyPenalty",id:"frequency-penalty"},{db:"seed",id:"seed"},{db:"showAdvanceFeature",id:"show-advance-feature"}]),i(e)}}}));export{C as default};
